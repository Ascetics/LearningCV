{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "np_data = np.arange(6).reshape((2, 3))\n",
    "torch_data = torch.tensor(np_data) # 张量\n",
    "tensor2array = torch_data.numpy()\n",
    "\n",
    "print(\n",
    "    '\\nnumpy array:\\n', np_data,\n",
    "    '\\ntorch tensor\\n', torch_data,\n",
    "    '\\ntensor to array\\n', tensor2array\n",
    ")\n",
    "\n",
    "# float 类型\n",
    "# torch.float16\n",
    "# torch.float32\n",
    "# torch.float64\n",
    "\n",
    "# int 类型\n",
    "# torch.uint8\n",
    "# torch.int8\n",
    "# torch.int16\n",
    "# torch.int32\n",
    "# torch.int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nnumpy.abs:\n [[1 2]\n [1 2]] \ntorch.abs:\n tensor([[1, 2],\n        [1, 2]]) \nnumpy.sin:\n [0.         0.70710678 1.        ] \ntorch.sin:\n tensor([0.0000, 0.7071, 1.0000], dtype=torch.float64) \nnumpy.mean\n 2.0 \ntorch.mean\n tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "np_data = np.array([[1, 2], [1, 2]])\n",
    "torch_data = torch.tensor(np_data)\n",
    "\n",
    "np_t = np.array([0, np.pi / 4., np.pi / 2.])\n",
    "torch_t = torch.tensor(np_t)\n",
    "\n",
    "# 平均值\n",
    "np_x = np.arange(5)\n",
    "torch_x = torch.FloatTensor([i for i in range(5)])\n",
    "# torch.mean 只能计算float的平均值，不能计算int的平均值，所有必须用FloatTensor\n",
    "\n",
    "print(\n",
    "    '\\nnumpy.abs:\\n', np.abs(np_data),\n",
    "    '\\ntorch.abs:\\n', torch.abs(torch_data),\n",
    "    '\\nnumpy.sin:\\n', np.sin(np_t),\n",
    "    '\\ntorch.sin:\\n', torch.sin(torch_t),\n",
    "    '\\nnumpy.mean\\n', np.mean(np_x),\n",
    "    '\\ntorch.mean\\n', torch.mean(torch_x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n [3 7]]\n[[1 2]\n [0 4]]\ntensor([[1., 3.],\n        [3., 7.]])\ntensor([[1., 2.],\n        [0., 4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# numpy矩阵相乘\n",
    "na = np.array([[1, 2], [3, 4]])\n",
    "nb = np.array([[1, 1], [0, 1]])\n",
    "nc = na @ nb  # nc = np.matmul(na, nb)\n",
    "nd = na * nb\n",
    "print(nc, nd, sep='\\n')\n",
    "\n",
    "# torch矩阵相乘\n",
    "ta = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "tb = torch.FloatTensor([[1, 1], [0, 1]])\n",
    "tc = torch.mm(ta, tb)\n",
    "td = ta * tb\n",
    "print(tc, td, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n        [3., 4.]])\ntensor([[1., 2.],\n        [3., 4.]], requires_grad=True)\ntensor(7.5000)\ntensor(7.5000, grad_fn=<MeanBackward0>)\ntensor([[0.5000, 1.0000],\n        [1.5000, 2.0000]])\ntensor([[1., 2.],\n        [3., 4.]], requires_grad=True)\ntensor([[1., 2.],\n        [3., 4.]])\n[[1. 2.]\n [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 变量\n",
    "tensor = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "variable = Variable(tensor, requires_grad=True)\n",
    "print(tensor)\n",
    "print(variable)\n",
    "t_mean = torch.mean(tensor * tensor)\n",
    "v_mean = torch.mean(variable * variable)\n",
    "print(t_mean)\n",
    "print(v_mean)\n",
    "\n",
    "# 反向传播求导数\n",
    "v_mean.backward()\n",
    "print(variable.grad)\n",
    "\n",
    "print(variable)  # Variable形式\n",
    "print(variable.data)  # tensor形式\n",
    "print(variable.data.numpy())  # numpy形式，随后输出结果一般用numpy形式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 网络的定义\n",
    "# torch.nn里面是类，类大写，多单词驼峰\n",
    "# torch.nn.functional里面是函数，函数小写，多单词下划线\n",
    "\n",
    "# 线性层（全连接层）\n",
    "nn.Linear(in_features, out_features, bias)\n",
    "F.linear(input, weight, bias)\n",
    "\n",
    "# 卷积层\n",
    "nn.Conv2d(in_channels=, out_channels=, kernel_size=,\n",
    "          stride=1, padding=0,\n",
    "          dilation=1,groups=1, bias=True, padding_mode='zeros')\n",
    "# dilation膨胀卷积\n",
    "\n",
    "# groups控制输入和输出之间的连接，可以减小参数\n",
    "# 28*28*2 3*3*64 --> 26*26*64                parm:2*3*3*64\n",
    "# 28*28*1 3*3*32 --> 26*26*32|___\\26*26*64   parm:1*3*3*32 + 1*3*3*32\n",
    "# 28*28*1 3*3*32 --> 26*26*32|   /\n",
    "F.conv2d(input, weight, bias=None, stride=1, padding=0,)\n",
    "\n",
    "# 转置卷积\n",
    "nn.ConvTranspose2d()\n",
    "F.conv_transpose2d()\n",
    "\n",
    "# 池化Average\n",
    "nn.AvgPool2d()\n",
    "F.avg_pool2d()\n",
    "\n",
    "# 池化层Max\n",
    "nn.MaxPool2d(return_indices=False, ceil_mode=False)\n",
    "# return_indices=True 返回池化最大位置坐标\n",
    "# ceil_mode=False, 5*5--2*2--2*2\n",
    "# ceil_mode=True, 5*5--2*2--3*3,如果池化除不尽向上取整\n",
    "# \n",
    "F.max_pool2d()\n",
    "F.max_pool2d_with_indices()\n",
    "\n",
    "# 反池化Max\n",
    "nn.MaxUnpool2d()\n",
    "F.max_unpool2d()\n",
    "\n",
    "# 分数化Max池化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# 内置数据集\n",
    "# workdir = os.getcwd()\n",
    "# torchvision.datasets.MNIST(\n",
    "#     root=workdir, \n",
    "#     train=True,\n",
    "#     transform=None, # 对data是否进行修改，比如Normalization\n",
    "#     target_transform=None, # 对label是否进行修改，比如1,2,...,9变成向量形式\n",
    "#     download=False # 是否下载\n",
    "# )\n",
    "\n",
    "root = os.path.dirname(os.getcwd())\n",
    "# d = torchvision.datasets.FashionMNIST(root, download=True)\n",
    "# print(root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# 自定义数据集\n",
    "# 继承 torch.utils.data.Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        必须实现这个方法，给len()函数用\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        pass\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        必须实现这个方法，给datasets[i]下标索引用\n",
    "        可以for迭代\n",
    "        :param item: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "# 另一种高级的迭代数据集的方式\n",
    "# torch.utils.data.DataLoader\n",
    "# num_workers 多线程\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nin_data\n tensor([[[[ 1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.],\n          [ 9., 10., 11., 12.],\n          [13., 14., 15., 16.]]]])\ntorch.Size([1, 1, 4, 4])\n\nout_data\n tensor([[[[ 6.,  8.],\n          [14., 16.]]]])\n\nindices\n tensor([[[[ 5,  7],\n          [13, 15]]]])\n\nun_data\n tensor([[[[ 0.,  0.,  0.,  0.],\n          [ 0.,  6.,  0.,  8.],\n          [ 0.,  0.,  0.,  0.],\n          [ 0., 14.,  0., 16.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# 最大池化与反池化\n",
    "pool = nn.MaxPool2d(\n",
    "    kernel_size=2,\n",
    "    stride=2,\n",
    "    return_indices=True,\n",
    "    ceil_mode=True\n",
    ")\n",
    "unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "in_data = Variable(\n",
    "    torch.Tensor([[[[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]]]])\n",
    "    # 为什么必须是1*1*4*4的维度呢\n",
    "    # 因为是N*C*H*W\n",
    ")\n",
    "print('\\nin_data\\n', in_data)\n",
    "print(in_data.shape)\n",
    "\n",
    "out_data, indices = pool(in_data)\n",
    "print('\\nout_data\\n', out_data)\n",
    "print('\\nindices\\n', indices)\n",
    "\n",
    "un_data = unpool(out_data, indices)\n",
    "print('\\nun_data\\n', un_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nin_data\n tensor([[[[ 1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.],\n          [ 9., 10., 11., 12.],\n          [13., 14., 15., 16.]]]])\ntorch.Size([1, 1, 4, 4])\n\nout_data\n tensor([[[[3.2893, 3.7151],\n          [4.9928, 5.4186]],\n\n         [[1.8688, 2.4146],\n          [4.0519, 4.5977]]]], grad_fn=<MkldnnConvolutionBackward>)\ntorch.Size([1, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# 卷积层举例\n",
    "k = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    kernel_size=3,\n",
    "    padding=0\n",
    ")\n",
    "in_data = Variable(\n",
    "    torch.Tensor([[[[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12],\n",
    "                    [13, 14, 15, 16]]]])\n",
    ")\n",
    "print('\\nin_data\\n', in_data)\n",
    "print(in_data.shape)\n",
    "\n",
    "out_data = k(in_data)\n",
    "print('\\nout_data\\n', out_data)\n",
    "print(out_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nin_data\n tensor([[-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,\n           2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.]])\ntorch.Size([1, 20])\n\nout_relu\n tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,\n         8., 9.]])\ntorch.Size([1, 20])\n\nout_relu6\n tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 6.,\n         6., 6.]])\ntorch.Size([1, 20])\n\nout_sigmoid\n tensor([[4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,\n         1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,\n         8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,\n         9.9966e-01, 9.9988e-01]])\ntorch.Size([1, 20])\n\nout_softmax\n tensor([[3.5416e-09, 9.6272e-09, 2.6169e-08, 7.1136e-08, 1.9337e-07, 5.2563e-07,\n         1.4288e-06, 3.8839e-06, 1.0557e-05, 2.8698e-05, 7.8010e-05, 2.1205e-04,\n         5.7642e-04, 1.5669e-03, 4.2592e-03, 1.1578e-02, 3.1471e-02, 8.5548e-02,\n         2.3254e-01, 6.3212e-01]])\ntorch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 激活函数  \n",
    "# inplace=True就是输出就赋值到输入对象上\n",
    "# nn.ReLU\n",
    "# nn.ReLU6\n",
    "# nn.Sigmoid\n",
    "# nn.LogSigmoid()\n",
    "# nn.Threshold()\n",
    "# nn.Tanh\n",
    "in_data = Variable(torch.Tensor([range(-10, 10)]))\n",
    "print('\\nin_data\\n', in_data)\n",
    "print(in_data.shape)\n",
    "\n",
    "out_relu = F.relu(in_data)\n",
    "print('\\nout_relu\\n', out_relu)\n",
    "print(out_relu.shape)\n",
    "\n",
    "# 为什么是relu6呢？这个6是个经验值，无语\n",
    "out_relu6 = F.relu6(in_data)\n",
    "print('\\nout_relu6\\n', out_relu6)\n",
    "print(out_relu6.shape)\n",
    "\n",
    "out_sigmoid = F.sigmoid(in_data)\n",
    "print('\\nout_sigmoid\\n', out_sigmoid)\n",
    "print(out_sigmoid.shape)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "out_softmax = softmax(in_data)\n",
    "print('\\nout_softmax\\n', out_softmax)\n",
    "print(out_softmax.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# BatchNormalization\n",
    "# momentum 指的是移动平均，之前值的参数\n",
    "# affine是否使用再学习参数\n",
    "nn.BatchNorm1d(num_features, eps=1e-5, momentum=0.1, affine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 损失函数\n",
    "nn.L1Loss\n",
    "F.l1_loss()\n",
    "\n",
    "nn.SmoothL1Loss\n",
    "F.smooth_l1_loss()\n",
    "\n",
    "nn.MSELoss # 平方差函数？\n",
    "F.mse_loss()\n",
    "\n",
    "nn.CrossEntropyLoss\n",
    "F.cross_entropy()\n",
    "\n",
    "nn.BCELoss\n",
    "F.binary_cross_entropy()\n",
    "\n",
    "# 最大似然函数\n",
    "nn.NLLLoss\n",
    "F.nll_loss()\n",
    "\n",
    "nn.NLLLoss2d\n",
    "F.nll_loss2d()\n",
    "\n",
    "nn.KLDivLoss\n",
    "F.kl_div()\n",
    "\n",
    "nn.HingeEmbeddingLoss\n",
    "F.hinge_embedding_loss()\n",
    "\n",
    "nn.CosineEmbeddingLoss\n",
    "F.cosine_embedding_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2.],\n          [3., 4.]]]])\ntensor([[[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 2., 1., 1.],\n          [1., 3., 4., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "in_data = Variable(torch.Tensor([[[[1, 2],\n",
    "                                   [3, 4]]]]))\n",
    "out_pad = F.pad(in_data, pad=[1, 2, 3, 4], value=1.)\n",
    "# 左pad=1，右pad=2，上pad=3，下pad=4\n",
    "print(in_data)\n",
    "print(out_pad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 优化器\n",
    "torch.optim.Optimizer(prams, defaluts)\n",
    "\n",
    "# lr learning rate\n",
    "torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-6, weight_decay=0)\n",
    "torch.optim.Adagrad\n",
    "torch.optim.Adam\n",
    "torch.optim.Adamax\n",
    "torch.optim.ASGD\n",
    "torch.optim.LBFGS\n",
    "torch.optim.RMSprop\n",
    "torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    \"\"\"\n",
    "    举例全连接模型\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_feature, n_hidden)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.predict = nn.Linear(n_hidden, n_output)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "linear_net = LinearNet(1, 10, 1)\n",
    "\n",
    "# 或者\n",
    "linear_net2 = nn.Sequential(\n",
    "    nn.Linear(1, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    举例卷积\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.predict = nn.Linear(32 * 7 * 7, 10)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)  # 相当于tensorflow里面的flatten操作\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "    pass\n",
    "\n",
    "\n",
    "cnn_model = CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
